# -*- coding: utf-8 -*-
"""Emotion_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19WMAHZ69a-eU4hHGpzk6b7Fr8xL948P5
"""

import pandas as pd
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

from google.colab import drive
drive.mount('/content/drive')
# Load the dataset
data = pd.read_csv('drive/MyDrive/emotions.csv')

# Display the first few rows to verify it loaded correctly
print(data.head())

data.shape

data.head()

"""# Data Preprocessing (Text Cleaning & Feature Engineering)"""

import pandas as pd
import re
import numpy as np
!pip install unidecode
from unidecode import unidecode
import string


import matplotlib.pyplot as plt
import seaborn as sns
import collections
from wordcloud import WordCloud

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer, WordNetLemmatizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, classification_report

nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")
nltk.download('punkt_tab')

# Initialize lemmatizer and stop words
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

# Function to clean and preprocess headlines
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r"\d+", "", text)  # Remove numbers
    text = text.translate(str.maketrans("", "", string.punctuation))  # Remove punctuation
    words = word_tokenize(text)  # Tokenization
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization
    return " ".join(words)

# Apply text cleaning to text
data["Cleaned_text"] = data["text"].apply(clean_text)

data.head()

"""# Exploratory Data Analysis (EDA)"""

# Display the first few rows to understand the structure
print("First 5 rows of the dataset:")
print(data.head())

"""a classification label, with possible values including sadness (0), joy (1), love (2), anger (3), fear (4) and surprise(5)"""

# Basic information about the dataset
print("\nDataset Info:")
data.info()

# Checking for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Summary statistics
print("\nSummary Statistics:")
print(data.describe(include='all'))

# Checking unique values in each colum
for column in data.columns:
    unique_values = data[column].nunique()
    print(f"\nUnique values in {column}: {unique_values}")

"""# Visualization"""

import seaborn as sns
import matplotlib.pyplot as plt

# Map numeric labels to emotion names
emotion_mapping = {
    0: 'Sadness',
    1: 'Joy',
    2: 'Love',
    3: 'Anger',
    4: 'Fear',
    5: 'Surprise'
}
data['label'] = data['label'].map(emotion_mapping)  # Replace labels with names

# Plot
plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='label')

plt.title("Distribution of Emotion Labels")
plt.xlabel("Emotion Label")
plt.ylabel("Count")
plt.show()

# ✅ Ensure the column is created
data["Cleaned_text_length"] = data["Cleaned_text"].apply(len)

# ✅ Now run the checks
print(data.head())  # Check data structure
print(data["label"].unique())  # Check unique labels
print(data["Cleaned_text_length"].describe())  # Summary of sentence lengths

import matplotlib.pyplot as plt
import seaborn as sns

# ✅ Plot the distribution using emotion names
plt.figure(figsize=(10, 5))

# Plot for each emotion
sns.histplot(data[data["label"] == "Sadness"]["Cleaned_text_length"], bins=30, label="Sadness", kde=True, color="blue", alpha=0.5)
sns.histplot(data[data["label"] == "Joy"]["Cleaned_text_length"], bins=30, label="Joy", kde=True, color="yellow", alpha=0.5)
sns.histplot(data[data["label"] == "Love"]["Cleaned_text_length"], bins=30, label="Love", kde=True, color="black", alpha=0.5)
sns.histplot(data[data["label"] == "Anger"]["Cleaned_text_length"], bins=30, label="Anger", kde=True, color="red", alpha=0.5)
sns.histplot(data[data["label"] == "Fear"]["Cleaned_text_length"], bins=30, label="Fear", kde=True, color="purple", alpha=0.5)
sns.histplot(data[data["label"] == "Surprise"]["Cleaned_text_length"], bins=30, label="Surprise", kde=True, color="green", alpha=0.5)

# ✅ Add labels and legend
plt.legend()
plt.title("Sentence Length Distribution by Emotion")
plt.xlabel("Sentence Length (Characters)")
plt.ylabel("Count")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# ✅ Generate Word Cloud using "Cleaned_text" column
text = " ".join(data["Cleaned_text"])

# ✅ Create the Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

# ✅ Plot the Word Cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Cleaned Text")
plt.show()

"""# Fixing the labels"""

import seaborn as sns
import matplotlib.pyplot as plt

# ✅ Map the label to a meaningful numeric value
emotion_mapping = {
    "Sadness": 0,
    "Joy": 1,
    "Love": 2,
    "Anger": 3,
    "Fear": 4,
    "Surprise": 5  # Added missing label
}

# ✅ Apply mapping
data["Sentiment_numeric"] = data["label"].map(emotion_mapping)

# ✅ Check for unmapped values
print(data["Sentiment_numeric"].isna().sum())  # Should be 0 if all mapped correctly

# ✅ Add a column for sentence length
data["Cleaned_text_length"] = data["Cleaned_text"].apply(len)

# ✅ Compute correlation
correlation = data[["Cleaned_text_length", "Sentiment_numeric"]].corr()

# ✅ Plot heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(correlation, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation between Sentence Length and Sentiment")
plt.show()

"""# Checking class Imbalance"""

import seaborn as sns
import matplotlib.pyplot as plt

# ✅ Visualizing the class imbalance using "label" column
sns.countplot(x='label', data=data, order=data['label'].value_counts().index, palette="viridis")
plt.title("Class Distribution: Emotion Categories")
plt.xlabel("Emotion Label")
plt.ylabel("Count")
plt.show()

# ✅ Displaying the percentage split
label_counts = data['label'].value_counts(normalize=True) * 100
print("\nPercentage Distribution:")
print(label_counts)

# ✅ Displaying the percentage split using "label" column
label_counts = data['label'].value_counts(normalize=True) * 100
print("\nPercentage Distribution:")
print(label_counts)

"""# Model Training

## **Splitting data**
"""

# ✅ Method 1
X = data['Cleaned_text']
y = data['label']

# ✅ Method 2
X = data.iloc[:, 2]  # Selects 'Cleaned_text'
y = data.iloc[:, 1]  # Selects 'label'

X

y

"""## **Checking unique values in y**"""

#method 1
y.unique()

#method 2
y.value_counts()

#method 3
sns.countplot(x = y)

"""## **Preprocessing**

### **Handling  Null**
"""

y.isnull().sum()

"""### **Emotions**"""

# ✅ Extract each emotion using the "label" column
sadness_data = data[data['label'] == 0]  # Sadness
joy_data = data[data['label'] == 1]      # Joy
love_data = data[data['label'] == 2]     # Love
anger_data = data[data['label'] == 3]    # Anger
fear_data = data[data['label'] == 4]     # Fear

# ✅ Display results
print("Sadness Data:")
print(sadness_data)

print("\nJoy Data:")
print(joy_data)

print("\nLove Data:")
print(love_data)

print("\nAnger Data:")
print(anger_data)

print("\nFear Data:")
print(fear_data)

"""## WordCloud for each emotion"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

sadness_data = data[data["label"] == "Sadness"]

def create_wordcloud(data, emotion_name):
    if data.empty:
        print(f"No data available for {emotion_name}, skipping word cloud.")
        return

    text = " ".join(data["Cleaned_text"])
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

    plt.figure(figsize=(8, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Word Cloud for {emotion_name}")
    plt.show()

create_wordcloud(sadness_data, "Sadness")

joy_data = data[data["label"] == "Joy"]
create_wordcloud(joy_data, "Joy")

love_data = data[data["label"] == "Love"]
create_wordcloud(love_data, "Love")

anger_data = data[data["label"] == "Anger"]
create_wordcloud(anger_data, "Anger")

fear_data = data[data["label"] == "Fear"]
create_wordcloud(fear_data, "Fear")

surprise_data = data[data["label"] == "Surprise"]
create_wordcloud(surprise_data, "Surprise")

"""# Vectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

# ✅ Use Cleaned_text for vectorization
X = data["Cleaned_text"].astype(str)  # Ensure it's a string

# ✅ Apply CountVectorizer
vec = CountVectorizer(stop_words='english')
X_vectorized = vec.fit_transform(X)  # Store transformed data separately

# ✅ Check shape of transformed data
print(X_vectorized.shape)  # (num_samples, num_features)

vec.vocabulary_

len(vec.vocabulary_) # to get len of all vocabulary

vec.vocabulary_['ashamed'] # to get no of vocabulary of particalar word

"""## Splitting"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=5)

"""## SMOTE analysis"""

# ✅ Install imbalanced-learn if not installed
!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd

# ✅ Use X_vectorized directly since vectorization is already done
y = data["label"]  # Use the existing label column

# ✅ Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

# ✅ Apply SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# ✅ Check the new class distribution
print("New class distribution:\n", pd.Series(y_resampled).value_counts())

sns.countplot(x = y)

"""## **Training model**"""

mnb = MultinomialNB()
mnb.fit(X_train,y_train)

mnb.score(X_train,y_train)

mnb.score(X_test,y_test)

mnb.classes_ #the sequence of confussion matrix

"""## Model Evaluation"""

y_pred = mnb.predict(X_test)
cm = confusion_matrix(y_pred,y_test)
cm

# Display accuracy and classification report
from sklearn.metrics import accuracy_score, classification_report

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Display accuracy and classification report
print(f"Model Accuracy: {accuracy:.4f}\n")
print("Classification Report:")
print(classification_report(y_test, y_pred))

ConfusionMatrixDisplay(cm, display_labels = mnb.classes_ ).plot()

# ✅ Emotion examples
emotion_examples = {
    "Sadness": "I feel so lonely.",
    "Joy": "I am extremely happy and excited today!",
    "Love": "I cherish and adore every moment with you.",
    "Anger": "I am so mad about this situation!",
    "Fear": "I'm terrified of what might happen next.",
    "Surprise": "I'm shocked!"
}

# ✅ Predicting Emotions
for emotion, sample_text in emotion_examples.items():
    # Vectorize the input
    sample_transformed = vec.transform([sample_text])

    # Predict the emotion
    predicted_label = mnb.predict(sample_transformed)[0]  # Directly a string label

    # No need for reverse mapping, directly compare
    predicted_emotion = predicted_label

    print(f"Input Text: {sample_text}")
    print(f"Expected Emotion: {emotion}")
    print(f"Predicted Emotion: {predicted_emotion}\n")

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
import joblib

# ✅ Example data including "Surprise"
X_train = [
    "I feel so lonely",       # Sadness
    "I am extremely happy",   # Joy
    "I cherish moments",      # Love
    "I am furious",           # Anger
    "I am scared",            # Fear
    "Wow, I did not see that coming!"  # Surprise
]

# ✅ Updated Labels
y_train = [0, 1, 2, 3, 4, 5]  # 0: Sadness, 1: Joy, 2: Love, 3: Anger, 4: Fear, 5: Surprise

# ✅ Train vectorizer and model
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)

model = MultinomialNB()
model.fit(X_train_vectorized, y_train)

# ✅ Save model and vectorizer
joblib.dump(model, "emotion_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

print("Model and vectorizer saved successfully!")